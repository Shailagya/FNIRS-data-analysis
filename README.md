# FNIRS-data-analysis
The study used a functional near-infrared spectroscopy (FNIRS) dataset to analyze baseline shifts and artifact spikes in the data.
Various methods were utilized to visualize and remove these unwanted components from the dataset.
Different techniques and approaches were used to remove the baseline shifts and artifact spikes.
The presentation will discuss the methods and approaches used to remove these unwanted components in further detail.

## Physiological Basis
->fNIRS is a non-invasive neuroimaging technique that measures changes in the concentration of oxygenated and deoxygenated hemoglobin in the brain.

->It is based on the neurovascular coupling hypothesis, which states that changes in neural activity cause changes in blood flow and oxygenation in the brain.

->When neurons become active, they consume more oxygen and glucose, leading to an increase in blood flow to that area to supply more oxygen and nutrients.

->fNIRS measures changes in hemoglobin concentrations by shining near-infrared light into the scalp, which interacts with hemoglobin molecules in the brain tissue.

->fNIRS data provides information about brain activation in response to specific tasks or stimuli and how these activations change over time.

## Reading and analysing the dataset
->The study worked with the 2023-02-15_001_MC_003 dataset, which included several files such as probeinfo.mat, the configuration file, the .wl1 and .wl2 files, the environment file, and most importantly, the .nirs file.

->The loadmat function was used to read the .nirs file, resulting in a numpy ndarray.

->The 'd' key within the data dictionary obtained from the loaded .nirs file was focused on to locate the NIRS data.

->The NIRS data was contained within this ndarray, as confirmed by plotting it and comparing it to the plot generated by the NIRS data visualizer.

![Plot](https://github.com/Shailagya/FNIRS-data-analysis/assets/137305675/094345bf-1ab4-4d8c-a4cf-8450e7c14483)

Plot of onw participant's data.

## Different methods used to detect & remove baseline shifts 
### 1.Using statistical test to detect if baseline shift is present 
We utilized the stats.ttest_rel function from the SciPy library to perform a paired t-test, which allowed us to compare the mean values of the first and second halves of the data. The t_statistic and p_value variables stored the results of this t-test.

To determine whether a baseline shift was present in the NIRS data, the code checked if the p-value was less than 0.05, a commonly used significance level. If the p-value was less than 0.05, the code printed "Baseline shift detected." On the other hand, if the p-value was greater than or equal to 0.05, the code printed "No baseline shift detected."

![baselineshift](https://github.com/Shailagya/FNIRS-data-analysis/assets/137305675/1e6c3b69-cebb-4b4c-a097-6c5e18f04c1b)

### 2.Using a quadratic polynomial fitting method
The following code performs baseline correction on a 36-channel NIRS dataset using a quadratic polynomial fitting method. Firstly, the necessary libraries, including NumPy for array manipulation, Matplotlib for plotting, and SciPy's signal processing module for polynomial fitting, are imported. 
The quadratic function baseline_func is defined with three parameters a, b, and c, which represent the coefficients of the quadratic function. 
To prepare for baseline correction, an array of time points is created for the NIRS data. Assuming the data is sampled at 10 Hz, the time points are calculated by dividing the number of samples by the sampling frequency. 
Next, two arrays are initialized - nirs_data_bc and baseline_shifts. The nirs_data_bc array stores the baseline-corrected data, while baseline_shifts will keep track of the amount of baseline shift for each channel.

For each channel, a quadratic function is fit to the baseline using np.polyfit. The fitted baseline is then subtracted from the original data, and the resulting baseline-corrected data is stored in nirs_data_bc. 
Finally, the code plots the amount of baseline shift for each channel as a bar graph, with the x-axis representing the channel number and the y-axis representing the amount of baseline shift. The resulting plot is saved with a suitable title and axis labels. 
Overall, this code provides a practical and efficient method for baseline correction in NIRS data analysis

![iiii](https://github.com/Shailagya/FNIRS-data-analysis/assets/137305675/16eb61cb-dc87-49de-a00f-15746048bbe8)
![yyy](https://github.com/Shailagya/FNIRS-data-analysis/assets/137305675/c6c029a2-0903-4381-aae6-b0b6aa15ed87)

### 3. Using Baseline Removal module-  Three Baseline removal algorithms used
ModPoly is a well-known baseline removal algorithm that uses a polynomial fitting approach to estimate and remove the baseline from a given signal.
The algorithm fits a polynomial curve to the signal's data points and subtracts it from the original signal to obtain a baseline-corrected signal.

IModPoly is an improved version of ModPoly that not only fits a polynomial curve to the signal but also applies a corrective filter to the original signal after subtracting the polynomial fit.
This filter helps to further reduce any remaining baseline drifts in the signal, and the polynomial degree is provided as an input to this algorithm.

ZhangFit is another popular baseline removal algorithm that uses cubic splines to fit a series of curves to the original signal's data points.
The algorithm then subtracts this spline fit from the original signal to obtain a baseline-corrected signal.
ZhangFit is particularly effective at removing the baseline from signals with complex or irregular baseline patterns.

Below is the comparison of the above 3 methods:

![compai](https://github.com/Shailagya/FNIRS-data-analysis/assets/137305675/e0183a0b-a0bd-4704-8c25-4f0493e48dbf)

### 4. Using rolling median for each channel
1. Define the window size and threshold for detecting baseline shifts. The window size determines the number of data points to use for the rolling median calculation. The threshold determines the level of deviation from the median that is considered a baseline shift.
2. Compute the rolling median for each window. The rolling median is a smoothed version of the original data that is less sensitive to outliers.
3. Compute the difference between the original data and the rolling median. This difference represents the amount of deviation from the baseline.
4. Detect baseline shifts exceeding the threshold. Baseline shifts are identified as data points where the deviation from the rolling median exceeds the defined threshold.
5. Plot the original data and baseline to visualize the shifts.
6. Plot the identified shifts as vertical lines on the same plot.
7. Correct the shifts by replacing the data with the corresponding baseline where shifts are detected.
![method4](https://github.com/Shailagya/FNIRS-data-analysis/assets/137305675/ef39db4f-6994-4cc8-a559-66ba5f19ac8a)


## Identifying The Peaks And Spikes And Removing Them From The Plots

### Method 1
->Load the NIRS data into a NumPy ndarray named 'data'.

->Calculate the median prominence of the signal peaks and set the 'height' parameter for peak detection to twice the median prominence.

->Detect peaks in the data using the 'find_peaks' function from the 'scipy.signal' module with the 'height' parameter.

->Plot the original data with the detected peaks marked for visualization.

->Correct the spikes by replacing them with the mean of surrounding data points using a specified window size.

![spikes](https://github.com/Shailagya/FNIRS-data-analysis/assets/137305675/332e4f83-cbe7-4b5f-a22e-c76a83f95342)

### 2. Findpeaks Method
The code performs two main steps to correct spikes in the data: peak detection and spike correction.
For peak detection, the 'find_peaks' function from Scipy.signal is used to detect local maxima in the data.
The prominence of each peak is calculated using the 'peak_prominences' function from Scipy.signal, and a threshold height is set based on the 95th percentile of the peak prominences.
The 'find_peaks' function is used again with the threshold height parameter to re-detect peaks, ensuring that only significant peaks are considered.
For spike correction, the code creates a copy of the original data array and iterates over each peak detected in the previous step.
For each peak, the left and right indices of the surrounding data points are determined (using a range of 10 data points to the left and right of the peak), and the mean of these points is calculated.
The peak value in the copied data array is then replaced with this mean value.
Finally, a figure is created using the 'matplotlib' library that displays the original and corrected data for the current iteration in two subplots arranged vertically. The original data is displayed in the top subplot, and the corrected data is displayed in the bottom subplot.

![corrected](https://github.com/Shailagya/FNIRS-data-analysis/assets/137305675/8d155a6e-4f64-4ac3-99d3-72c1877ad773)

